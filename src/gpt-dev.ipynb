{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "import os\n",
    "\n",
    "# Download URL, the shakespeare.txt\n",
    "url = f'https://drive.google.com/uc?id=1O4PZ8wOpp6yecoy8tMuVEIFS7XgyRJy9'\n",
    "\n",
    "data_path = '../data'\n",
    "text_path = f'{data_path}/shakespeare.txt'\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path)\n",
    "\n",
    "if not os.path.exists(text_path):\n",
    "  gdown.download(url, text_path, quiet=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lenth of the text 5433453\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "with open(text_path) as f:\n",
    "  text = f.read()\n",
    "  \n",
    "text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "print(f\"lenth of the text {len(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But as the riper should by time decease,\n",
      "  His tender heir might bear his memory:\n",
      "  But thou contracted to thine own bright eyes,\n",
      "  Feed'st thy light's flame with self-substantial fuel,\n",
      "  Making a famine where abundance lies,\n",
      "  Thy self thy foe, to thy sweet self too cruel:\n",
      "  Thou that art now the world's fresh ornament,\n",
      "  And only herald to the gaudy spring,\n",
      "  Within thine own bud buriest thy content,\n",
      "  And tender churl mak'st waste in niggarding:\n",
      "    Pity the world, or else this glutton be,\n",
      "    To eat the world's due, by the grave and thee.\n",
      "\n",
      "\n",
      "                     \n",
      "  When forty winters shall besiege thy brow,\n",
      "  And dig deep trenches in thy beauty's field,\n",
      "  Thy youth's proud livery so gazed on now,\n",
      "  Will be a tattered weed of small worth held:\n",
      "  Then being asked, where all thy beauty lies,\n",
      "  Where all the treasure of thy lusty days;\n",
      "  To say within thine own deep sunken eyes,\n",
      "  Were an all-ea\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size : 74\n",
      "\n",
      " !\"&'(),-.:;<>?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_`abcdefghijklmnopqrstuvwxyz|}\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(f\"vocab size : {len(chars)}\")\n",
    "print(\"\".join(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tiktoken\n",
    "# tokenizer = tiktoken.get_encoding('gpt2')\n",
    "# tokens = tokenizer.encode(text)\n",
    "# print(f\"total tokens {len(tokens)}\")\n",
    "# print(\"decode result of \\\"hello world.\\\"\", tokenizer.decode([31373, 995]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "  def __init__(self, text):\n",
    "    self.chars = sorted(list(set(text)))\n",
    "    self.token2id = {c : i for i, c in enumerate(chars)}\n",
    "    self.id2token = {i : c for i, c in enumerate(chars)}\n",
    "    \n",
    "  def encode(self, text):\n",
    "    return [self.token2id[c] for c in text]\n",
    "  \n",
    "  def decode(self, token_ids):\n",
    "    return \"\".join([self.id2token[token_id] for token_id in token_ids])\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23, 50, 57, 57, 60]\n",
      "Hello\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizer(text)\n",
    "vocab_size = len(tokenizer.chars)\n",
    "print(\n",
    "  tokenizer.encode(\"Hello\"),\n",
    "  tokenizer.decode([23, 50, 57, 57, 60]),\n",
    "  sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data for torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1,  1, 21,  ..., 29, 19,  0], dtype=torch.int8) torch.Size([5433453]) torch.int8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data = torch.tensor(tokenizer.encode(text), dtype = torch.int8) # Be careful about the type, which should hold the tokens\n",
    "print(data, data.shape, data.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_size = int(data.shape[0] * 0.9)\n",
    "train_data = data[:train_data_size].detach()\n",
    "val_data = data[train_data_size:].detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class SimpleDataset(Dataset):\n",
    "  def __init__(self, data, block_size = 8):\n",
    "    self.data = data\n",
    "    self.block_size = block_size\n",
    "    \n",
    "  def __len__(self):\n",
    "    return len(self.data) - self.block_size\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    x = self.data[idx: idx + self.block_size]\n",
    "    y = self.data[idx + self.block_size]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SimpleDataset(train_data)\n",
    "val_dataset = SimpleDataset(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4890099 543338\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset), len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  1, 21, 63, 60, 58,  1, 51, 46], dtype=torch.int8)"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 1,  1, 21, 63, 60, 58,  1, 51], dtype=torch.int8),\n",
       " tensor(46, dtype=torch.int8))"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class SimpleDataloader(DataLoader):\n",
    "  def __init__(self, dataset, batch_size=4, shuffle=True, **kwargs):\n",
    "    super().__init__(dataset, batch_size=batch_size, shuffle=shuffle, **kwargs)\n",
    "    self.shuffle = shuffle\n",
    "  def __iter__(self):\n",
    "    dataset_size = len(self.dataset)\n",
    "    indices = np.arange(dataset_size)\n",
    "    if self.shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "    for start_idx in range(0, dataset_size - self.batch_size + 1, self.batch_size):\n",
    "        batch_indices = indices[start_idx:start_idx + self.batch_size]\n",
    "        yield (torch.stack([self.dataset[i][0] for i in batch_indices]),\n",
    "              torch.stack([self.dataset[i][1] for i in batch_indices]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "train_dataloader = SimpleDataloader(train_dataset)\n",
    "val_dataloader = SimpleDataloader(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[50,  1, 51, 54, 67, 50,  1, 53],\n",
      "        [53, 46, 65, 53,  1, 60, 51, 51],\n",
      "        [68, 46, 54, 57,  1, 54, 59, 50],\n",
      "        [34, 20, 35,  0,  0,  1,  1, 34]], dtype=torch.int8), tensor([66, 50, 67, 30], dtype=torch.int8))\n",
      "(tensor([[70,  1, 48, 46, 63, 50,  1, 49],\n",
      "        [60, 51,  1, 29, 46, 61, 57, 50],\n",
      "        [70,  1, 57, 60, 63, 49, 10,  1],\n",
      "        [47, 50, 46, 66, 65, 54, 50, 64]], dtype=torch.int8), tensor([63, 64, 24,  1], dtype=torch.int8))\n",
      "(tensor([[ 1, 58, 50, 10,  1,  1,  1,  1],\n",
      "        [64,  0,  1,  1,  1,  1, 30, 51],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [66, 63,  1, 54, 59,  1, 58, 46]], dtype=torch.int8), tensor([ 1,  1, 35, 63], dtype=torch.int8))\n",
      "(tensor([[ 1, 65, 53, 70,  1, 61, 60, 48],\n",
      "        [51, 60, 63, 48, 50, 49,  1, 65],\n",
      "        [59, 52,  5, 64,  1, 64, 60, 59],\n",
      "        [ 1, 16, 31, 20, 28, 16, 29, 35]], dtype=torch.int8), tensor([56, 60,  1, 36], dtype=torch.int8))\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(train_dataloader):\n",
    "  print(batch)\n",
    "  if i == 3:\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
