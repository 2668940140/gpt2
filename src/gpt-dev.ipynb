{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "import os\n",
    "\n",
    "# Download URL, the shakespeare.txt\n",
    "url = f'https://drive.google.com/uc?id=1O4PZ8wOpp6yecoy8tMuVEIFS7XgyRJy9'\n",
    "\n",
    "data_path = '../data'\n",
    "text_path = f'{data_path}/shakespeare.txt'\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path)\n",
    "\n",
    "if not os.path.exists(text_path):\n",
    "  gdown.download(url, text_path, quiet=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lenth of the text 5046489\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "with open(text_path) as f:\n",
    "  text = f.read()\n",
    "  \n",
    "text = re.sub(r'\\d+', '', text)\n",
    "text = re.sub(r' +', ' ', text)\n",
    "\n",
    "print(f\"lenth of the text {len(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " From fairest creatures we desire increase,\n",
      " That thereby beauty's rose might never die,\n",
      " But as the riper should by time decease,\n",
      " His tender heir might bear his memory:\n",
      " But thou contracted to thine own bright eyes,\n",
      " Feed'st thy light's flame with self-substantial fuel,\n",
      " Making a famine where abundance lies,\n",
      " Thy self thy foe, to thy sweet self too cruel:\n",
      " Thou that art now the world's fresh ornament,\n",
      " And only herald to the gaudy spring,\n",
      " Within thine own bud buriest thy content,\n",
      " And tender churl mak'st waste in niggarding:\n",
      " Pity the world, or else this glutton be,\n",
      " To eat the world's due, by the grave and thee.\n",
      "\n",
      "\n",
      " \n",
      " When forty winters shall besiege thy brow,\n",
      " And dig deep trenches in thy beauty's field,\n",
      " Thy youth's proud livery so gazed on now,\n",
      " Will be a tattered weed of small worth held:\n",
      " Then being asked, where all thy beauty lies,\n",
      " Where all the treasure of thy lusty days;\n",
      " To say within thine own deep sunken eyes,\n",
      " Were an all-eating shame, and thriftless praise.\n",
      " How much m\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size : 74\n",
      "\n",
      " !\"&'(),-.:;<>?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_`abcdefghijklmnopqrstuvwxyz|}\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(f\"vocab size : {len(chars)}\")\n",
    "print(\"\".join(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tiktoken\n",
    "# tokenizer = tiktoken.get_encoding('gpt2')\n",
    "# tokens = tokenizer.encode(text)\n",
    "# print(f\"total tokens {len(tokens)}\")\n",
    "# print(\"decode result of \\\"hello world.\\\"\", tokenizer.decode([31373, 995]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "  def __init__(self, text):\n",
    "    self.chars = sorted(list(set(text)))\n",
    "    self.token2id = {c : i for i, c in enumerate(chars)}\n",
    "    self.id2token = {i : c for i, c in enumerate(chars)}\n",
    "    \n",
    "  def encode(self, text):\n",
    "    return [self.token2id[c] for c in text]\n",
    "  \n",
    "  def decode(self, token_ids):\n",
    "    return \"\".join([self.id2token[token_id] for token_id in token_ids])\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23, 50, 57, 57, 60]\n",
      "Hello\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizer(text)\n",
    "vocab_size = len(tokenizer.chars)\n",
    "print(\n",
    "  tokenizer.encode(\"Hello\"),\n",
    "  tokenizer.decode([23, 50, 57, 57, 60]),\n",
    "  sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data for torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1, 21, 63,  ..., 29, 19,  0], device='cuda:0') torch.Size([5046489]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data = torch.tensor(tokenizer.encode(text), dtype = torch.long,\n",
    "                    device=device) # torch.long can be used as index directly\n",
    "print(data, data.shape, data.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_size = int(data.shape[0] * 0.9)\n",
    "train_data = data[:train_data_size].detach()\n",
    "val_data = data[train_data_size:].detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class SimpleDataset(Dataset):\n",
    "  def __init__(self, data, block_size = 8):\n",
    "    self.data = data\n",
    "    self.block_size = block_size\n",
    "    \n",
    "  def __len__(self):\n",
    "    return len(self.data) - self.block_size\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    x = self.data[idx: idx + self.block_size]\n",
    "    y = self.data[idx + self.block_size]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_dataset = SimpleDataset(train_data)\n",
    "val_dataset = SimpleDataset(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4541832 504641\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset), len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1, 21, 63, 60, 58,  1, 51, 46, 54], device='cuda:0')"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 1, 21, 63, 60, 58,  1, 51, 46], device='cuda:0'),\n",
       " tensor(54, device='cuda:0'))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class SimpleDataloader(DataLoader):\n",
    "  def __init__(self, dataset, batch_size=4, shuffle=True, **kwargs):\n",
    "    super().__init__(dataset, batch_size=batch_size, shuffle=shuffle, **kwargs)\n",
    "    self.shuffle = shuffle\n",
    "    \n",
    "  def __iter__(self):\n",
    "    dataset_size = len(self.dataset)\n",
    "    indices = np.arange(dataset_size)\n",
    "    if self.shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "    for start_idx in range(0, dataset_size - self.batch_size + 1, self.batch_size):\n",
    "        batch_indices = indices[start_idx:start_idx + self.batch_size]\n",
    "        yield (torch.stack([self.dataset[i][0] for i in batch_indices]).to(device),\n",
    "              torch.stack([self.dataset[i][1] for i in batch_indices]).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(0)\n",
    "train_dataloader = SimpleDataloader(train_dataset, BATCH_SIZE)\n",
    "val_dataloader = SimpleDataloader(val_dataset, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[33, 19, 10,  1, 38, 50, 57, 57],\n",
      "        [53, 50,  1, 68, 54, 57, 57, 10],\n",
      "        [64,  1, 53, 50,  1, 58, 70,  1],\n",
      "        [50, 59,  1, 68, 54, 65, 53,  1],\n",
      "        [ 0,  1, 26, 54, 59, 64, 58, 50],\n",
      "        [66, 64,  1, 65, 60,  1, 53, 54],\n",
      "        [63, 54, 46, 57,  1, 60, 51,  1],\n",
      "        [53, 10,  1, 38, 53, 70,  1, 54],\n",
      "        [60,  1, 53, 54, 58, 64, 50, 57],\n",
      "        [65,  1, 70, 60, 66, 63,  1, 22],\n",
      "        [46, 52, 50, 10,  0,  1, 30, 33],\n",
      "        [54, 59, 52,  0,  1, 17, 66, 65],\n",
      "        [60, 51,  1, 65, 53, 50,  1, 68],\n",
      "        [64, 65,  1, 58, 70,  1, 68, 54],\n",
      "        [58, 12,  1, 53, 60, 57, 49,  1],\n",
      "        [ 1, 28, 46, 55, 50, 64, 65, 70],\n",
      "        [66,  1, 58, 50, 63, 63, 70,  2],\n",
      "        [66, 59, 52,  1, 40, 60, 63, 56],\n",
      "        [ 1, 67, 60, 54, 48, 50, 64, 15],\n",
      "        [ 1, 65, 53, 46, 65,  1, 53, 50],\n",
      "        [52,  5, 49,  8,  1, 24,  5, 57],\n",
      "        [38, 53, 60, 64, 50,  1, 67, 54],\n",
      "        [ 1, 52, 63, 50, 46, 65,  1, 59],\n",
      "        [35, 23, 20,  1, 20, 29, 19,  0],\n",
      "        [47, 70,  1, 58, 50,  1, 68, 53],\n",
      "        [53, 60, 57, 49,  1, 46,  1, 51],\n",
      "        [ 1, 27, 66, 48, 54, 66, 64, 12],\n",
      "        [63, 49,  8,  1, 24,  1, 53, 60],\n",
      "        [49, 50,  1, 46, 59, 49,  1, 68],\n",
      "        [64, 61, 60, 64,  5, 49,  1, 65],\n",
      "        [64, 60, 57, 67, 50,  1, 58, 50],\n",
      "        [46, 58, 59,  5, 49,  1, 51, 60]], device='cuda:0'), tensor([ 8,  0, 49, 64, 59, 58, 65, 64, 51, 63, 27,  1, 54, 57, 66,  0,  1, 15,\n",
      "         0,  1, 57, 60, 50,  0, 54, 46,  0, 57, 63, 60,  1, 63],\n",
      "       device='cuda:0'))\n",
      "(tensor([[58, 46, 64, 65, 50, 63,  8,  1],\n",
      "        [66, 47, 57, 50, 12,  0,  1, 16],\n",
      "        [ 1, 52, 63, 60, 46, 59, 54, 59],\n",
      "        [60, 60, 58,  1, 65, 53, 50, 50],\n",
      "        [24, 31, 23, 30, 27, 36, 34,  1],\n",
      "        [22,  1, 23, 20, 29, 33, 40, 10],\n",
      "        [50, 49,  1, 68, 60, 58, 46, 59],\n",
      "        [ 1, 53, 54, 58,  1, 48, 60, 58],\n",
      "        [ 1, 68, 46, 70, 64,  1, 65, 60],\n",
      "        [65, 12,  0,  1, 35, 53, 70,  1],\n",
      "        [50, 46, 64, 65, 10,  0,  1, 42],\n",
      "        [50, 63, 46, 59, 48, 50,  1, 58],\n",
      "        [70, 60, 66, 63,  1, 64, 61, 50],\n",
      "        [64, 47, 66, 63, 70,  8,  1, 46],\n",
      "        [64, 65, 50, 63, 64,  1, 60, 51],\n",
      "        [40,  0,  0,  1, 34, 50, 50,  8],\n",
      "        [66,  1, 68, 60, 66, 57, 49, 64],\n",
      "        [ 1, 68, 60, 63, 65, 53, 70,  1],\n",
      "        [50,  1, 68, 46, 64,  1, 48, 63],\n",
      "        [50, 48, 65, 50, 49,  1, 57, 54],\n",
      "        [60, 66,  8,  0,  1, 30, 51,  1],\n",
      "        [65,  0,  1, 49, 63, 50, 46, 49],\n",
      "        [ 9, 50, 59, 65, 50, 63,  1, 18],\n",
      "        [24, 22, 23, 35,  1,  9,  1, 17],\n",
      "        [60, 57, 49,  1, 58, 46, 59, 12],\n",
      "        [ 1, 29, 60,  8,  1, 53, 46, 65],\n",
      "        [46, 64, 65, 50,  1, 68, 53, 50],\n",
      "        [59, 49,  0,  1, 65, 53, 50,  1],\n",
      "        [ 0,  0,  1,  5, 24, 51,  1, 65],\n",
      "        [48, 53,  1, 53, 50, 63,  1, 68],\n",
      "        [63, 50, 54, 52, 59,  1, 53, 46],\n",
      "        [46, 59, 49,  1, 61, 63, 60, 67]], device='cuda:0'), tensor([46, 59, 52,  1, 30,  1,  2, 61,  1, 50, 23, 60, 46, 59,  1,  1, 65, 35,\n",
      "        46, 56, 64, 51, 27, 40,  1, 53, 63, 64, 53, 53, 65, 60],\n",
      "       device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(train_dataloader):\n",
    "  print(batch)\n",
    "  if i == 1:\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Configuration\n",
    "VOCAB_SIZE = 74 # Should be set according to the tokenizer\n",
    "EMBED_DIM = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleGPT(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    # (B, T)\n",
    "    self.embedding = nn.Embedding(VOCAB_SIZE, VOCAB_SIZE) #(B, T, T)\n",
    "    \n",
    "\n",
    "  def forward(self, x, targets = None):\n",
    "    \"\"\"\n",
    "    x should be in the form of (B, T)\n",
    "    \"\"\"\n",
    "    logits = self.embedding(x)\n",
    "    y = logits[:, -1,:]\n",
    "    y = F.softmax(y, 1)\n",
    "\n",
    "    if targets is None:\n",
    "      loss = None\n",
    "    else:\n",
    "      targets_one_hot = F.one_hot(targets, VOCAB_SIZE).float()\n",
    "      loss = F.cross_entropy(y, targets_one_hot)\n",
    "\n",
    "    return y, loss\n",
    "  \n",
    "  def generate(self, x, max_tokens = 30):\n",
    "    \"\"\"\n",
    "    x (B, T)\n",
    "\n",
    "    ### returns\n",
    "    y (B, T + max_tokens)\n",
    "\n",
    "    ### Warning\n",
    "    Because we do not have an EOF, so it will generate max_tokens actually\n",
    "    \"\"\"\n",
    "    for _ in range(max_tokens):\n",
    "      # x shape (B, T)\n",
    "      probs, loss = self.forward(x) # y shape (B, vocab_size)\n",
    "      y = torch.multinomial(probs, num_samples=1)\n",
    "      x = torch.cat([x, y], dim = 1)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = SimpleGPT().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f17cd739bd0>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nM\\nnWR!FYcaB}?x ?P!Mb cqIl,nbI},U\"QQb:x[RnKY}.bd]n,MgKy,t_Rr\"\\'U`|zRqZGxC\"GYwM\\'_HdUl[!d hEwVJ;i`VAF e;F;FTtVfgbI[n|}A?gaYnEvNm}msyku|r<Htc IyP,?(,HtVFP]vDx??\\n`pSto`MKZd>Hw>D!oPIK[iUfOmS&\"eTj`oYSQ.N<_RFR'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mubble(max_tokens = 200):\n",
    "  input = torch.zeros(1, 1, dtype=torch.long,device=device)\n",
    "  output = gpt.generate(input, max_tokens)\n",
    "  text = tokenizer.decode(output[0].tolist())\n",
    "  return text\n",
    "\n",
    "mubble()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(gpt.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch(eval_n = 100):\n",
    "  for i, batch in enumerate(train_dataloader):\n",
    "    gpt.train()\n",
    "    X, targets = batch\n",
    "    logits, loss = gpt.forward(X, targets)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "      gpt.eval()\n",
    "      tot_loss = 0\n",
    "      for j, batch in enumerate(val_dataloader):\n",
    "        X, targets = batch\n",
    "        logits, loss = gpt.forward(X, targets)\n",
    "        tot_loss += loss.sum()\n",
    "        if j == eval_n:\n",
    "          break\n",
    "      ave_loss = tot_loss / eval_n\n",
    "      print(ave_loss)\n",
    "    \n",
    "    if i == 10000:\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.3475, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(4.3354, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(4.3087, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(4.2675, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(4.2319, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(4.1988, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(4.1806, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(4.1626, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(4.1483, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(4.1417, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(4.1393, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "epoch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " the the the the the the the che the the the hepe the the the QThe the the the the the the ous t thand the ou the the the the toul, the the the the the the the he thouris the s the the he the the the \n"
     ]
    }
   ],
   "source": [
    "print(mubble())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
