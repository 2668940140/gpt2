{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "import os\n",
    "\n",
    "# Download URL, the shakespeare.txt\n",
    "url = f'https://drive.google.com/uc?id=1O4PZ8wOpp6yecoy8tMuVEIFS7XgyRJy9'\n",
    "\n",
    "data_path = '../data'\n",
    "text_path = f'{data_path}/shakespeare.txt'\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path)\n",
    "\n",
    "if not os.path.exists(text_path):\n",
    "  gdown.download(url, text_path, quiet=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lenth of the text 5046489\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "with open(text_path) as f:\n",
    "  text = f.read()\n",
    "  \n",
    "text = re.sub(r'\\d+', '', text)\n",
    "text = re.sub(r' +', ' ', text)\n",
    "\n",
    "print(f\"lenth of the text {len(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " From fairest creatures we desire increase,\n",
      " That thereby beauty's rose might never die,\n",
      " But as the riper should by time decease,\n",
      " His tender heir might bear his memory:\n",
      " But thou contracted to thine own bright eyes,\n",
      " Feed'st thy light's flame with self-substantial fuel,\n",
      " Making a famine where abundance lies,\n",
      " Thy self thy foe, to thy sweet self too cruel:\n",
      " Thou that art now the world's fresh ornament,\n",
      " And only herald to the gaudy spring,\n",
      " Within thine own bud buriest thy content,\n",
      " And tender churl mak'st waste in niggarding:\n",
      " Pity the world, or else this glutton be,\n",
      " To eat the world's due, by the grave and thee.\n",
      "\n",
      "\n",
      " \n",
      " When forty winters shall besiege thy brow,\n",
      " And dig deep trenches in thy beauty's field,\n",
      " Thy youth's proud livery so gazed on now,\n",
      " Will be a tattered weed of small worth held:\n",
      " Then being asked, where all thy beauty lies,\n",
      " Where all the treasure of thy lusty days;\n",
      " To say within thine own deep sunken eyes,\n",
      " Were an all-eating shame, and thriftless praise.\n",
      " How much m\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size : 74\n",
      "\n",
      " !\"&'(),-.:;<>?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_`abcdefghijklmnopqrstuvwxyz|}\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(f\"vocab size : {len(chars)}\")\n",
    "print(\"\".join(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tiktoken\n",
    "# tokenizer = tiktoken.get_encoding('gpt2')\n",
    "# tokens = tokenizer.encode(text)\n",
    "# print(f\"total tokens {len(tokens)}\")\n",
    "# print(\"decode result of \\\"hello world.\\\"\", tokenizer.decode([31373, 995]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "  def __init__(self, text):\n",
    "    self.chars = sorted(list(set(text)))\n",
    "    self.token2id = {c : i for i, c in enumerate(chars)}\n",
    "    self.id2token = {i : c for i, c in enumerate(chars)}\n",
    "    \n",
    "  def encode(self, text):\n",
    "    return [self.token2id[c] for c in text]\n",
    "  \n",
    "  def decode(self, token_ids):\n",
    "    return \"\".join([self.id2token[token_id] for token_id in token_ids])\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23, 50, 57, 57, 60]\n",
      "Hello\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizer(text)\n",
    "vocab_size = len(tokenizer.chars)\n",
    "print(\n",
    "  tokenizer.encode(\"Hello\"),\n",
    "  tokenizer.decode([23, 50, 57, 57, 60]),\n",
    "  sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data for torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1, 21, 63,  ..., 29, 19,  0], device='cuda:0') torch.Size([5046489]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data = torch.tensor(tokenizer.encode(text), dtype = torch.long,\n",
    "                    device=device) # torch.long can be used as index directly\n",
    "print(data, data.shape, data.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_size = int(data.shape[0] * 0.9)\n",
    "train_data = data[:train_data_size].detach()\n",
    "val_data = data[train_data_size:].detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class SimpleDataset(Dataset):\n",
    "  def __init__(self, data, block_size = 8):\n",
    "    self.data = data\n",
    "    self.block_size = block_size\n",
    "    \n",
    "  def __len__(self):\n",
    "    return len(self.data) - self.block_size\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    x = self.data[idx: idx + self.block_size]\n",
    "    y = self.data[idx + self.block_size]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_dataset = SimpleDataset(train_data)\n",
    "val_dataset = SimpleDataset(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4541832 504641\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset), len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1, 21, 63, 60, 58,  1, 51, 46, 54], device='cuda:0')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 1, 21, 63, 60, 58,  1, 51, 46], device='cuda:0'),\n",
       " tensor(54, device='cuda:0'))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class SimpleDataloader(DataLoader):\n",
    "  def __init__(self, dataset, batch_size=4, shuffle=True, **kwargs):\n",
    "    super().__init__(dataset, batch_size=batch_size, shuffle=shuffle, **kwargs)\n",
    "    self.shuffle = shuffle\n",
    "    \n",
    "  def __iter__(self):\n",
    "    dataset_size = len(self.dataset)\n",
    "    indices = np.arange(dataset_size)\n",
    "    if self.shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "    for start_idx in range(0, dataset_size - self.batch_size + 1, self.batch_size):\n",
    "        batch_indices = indices[start_idx:start_idx + self.batch_size]\n",
    "        yield (torch.stack([self.dataset[i][0] for i in batch_indices]).to(device),\n",
    "              torch.stack([self.dataset[i][1] for i in batch_indices]).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(0)\n",
    "train_dataloader = SimpleDataloader(train_dataset, BATCH_SIZE)\n",
    "val_dataloader = SimpleDataloader(val_dataset, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[63,  1, 68, 60, 63, 49, 64,  1],\n",
      "        [65, 53, 50,  1, 60, 46, 56,  8],\n",
      "        [68, 54, 57, 57,  1, 70, 60, 66],\n",
      "        [57, 54, 52, 53, 65, 64,  1, 60],\n",
      "        [57, 49,  1, 59, 60, 65,  1, 64],\n",
      "        [57, 70,  1, 50, 59, 49, 50, 46],\n",
      "        [63, 70,  1, 53, 50, 46, 63, 65],\n",
      "        [70,  8,  1, 59, 46, 70,  8,  1],\n",
      "        [59, 49,  1, 47, 60, 60, 65,  2],\n",
      "        [64,  1, 58, 50, 12,  1, 53, 50],\n",
      "        [58, 54, 59, 50,  1, 50, 70, 50],\n",
      "        [16, 36, 29, 18, 20, 27, 30, 35],\n",
      "        [46, 52, 50, 59, 50, 65,  8,  1],\n",
      "        [47, 57, 50,  1, 47, 46, 49, 59],\n",
      "        [51, 60, 63,  1, 57, 50, 46, 49],\n",
      "        [46,  1, 58, 46, 59,  1, 65, 60],\n",
      "        [50,  1, 48, 53, 46, 65, 10,  0],\n",
      "        [34, 35, 20, 33, 10,  1, 23, 60],\n",
      "        [56,  1, 57, 54, 56, 50, 68, 54],\n",
      "        [40,  1, 38, 30, 33, 27, 19,  1],\n",
      "        [61, 63, 46, 70,  1, 70, 60, 66],\n",
      "        [50,  1, 53, 46, 65, 53,  1, 64],\n",
      "        [50, 59, 65, 60, 58, 47,  5, 49],\n",
      "        [64, 65,  7,  1, 53, 60, 57, 49],\n",
      "        [ 5, 24,  1, 68, 60, 66, 57, 49],\n",
      "        [61, 63, 46, 70, 50, 63, 64,  8],\n",
      "        [ 1, 54, 59,  1, 46, 57, 57,  1],\n",
      "        [51, 50, 50, 49,  1, 65, 53, 50],\n",
      "        [49,  9,  1, 53, 60, 68,  1, 54],\n",
      "        [ 1, 47, 63, 60, 46, 49, 50, 63],\n",
      "        [70, 60, 66,  1, 61, 57, 46, 70],\n",
      "        [37, 16, 29, 35, 10,  1, 34, 54]], device='cuda:0'), tensor([65,  1,  1,  5, 66, 63,  2, 30,  0, 63,  8, 10, 57, 50, 10,  1,  1, 68,\n",
      "        64, 27,  8, 48,  1, 64,  1,  1, 49, 54, 64,  1,  1, 63],\n",
      "       device='cuda:0'))\n",
      "(tensor([[10,  1, 28, 46, 63, 63, 70,  8],\n",
      "        [64,  1, 65, 53, 50,  1, 60, 46],\n",
      "        [64, 60,  1, 64, 66, 49, 49, 50],\n",
      "        [50, 64, 65, 10,  1, 24,  1, 61],\n",
      "        [52, 58, 70,  5, 64,  1, 64, 65],\n",
      "        [ 0,  1, 24, 59, 51, 66, 64,  5],\n",
      "        [53, 50,  1, 46, 59, 49,  1, 24],\n",
      "        [ 1, 29, 50, 61, 65, 66, 59, 50],\n",
      "        [66, 63, 54, 64, 53,  1, 65, 53],\n",
      "        [ 1, 46, 64,  1, 51, 63, 50, 64],\n",
      "        [59, 60, 65,  1, 65, 60,  1, 47],\n",
      "        [60, 57, 49,  8,  0,  1, 23, 50],\n",
      "        [54, 57, 57, 50, 49,  1, 66, 61],\n",
      "        [20, 16, 33, 20,  1, 24, 34,  1],\n",
      "        [ 1, 30, 51,  1, 46, 54, 49, 64],\n",
      "        [ 1, 60, 51,  1, 53, 54, 64,  1],\n",
      "        [16,  1, 27, 46, 59, 48, 46, 64],\n",
      "        [ 1, 33, 60, 68, 57, 46, 59, 49],\n",
      "        [46, 64,  1, 24,  1, 53, 46, 67],\n",
      "        [29, 34, 10,  1, 17, 70,  1, 65],\n",
      "        [66, 64,  8,  0,  1, 30, 59, 50],\n",
      "        [60,  1, 46, 65, 60, 59, 50,  1],\n",
      "        [ 1, 65, 53, 50,  1, 53, 60, 57],\n",
      "        [58, 50, 59, 65, 10,  0,  1, 24],\n",
      "        [58, 50, 50, 65,  1, 53, 50, 63],\n",
      "        [ 1, 65, 53, 63, 50, 50,  1, 70],\n",
      "        [10,  0, 42, 34, 65, 63, 54, 56],\n",
      "        [54, 59, 65, 50, 57, 57, 50, 48],\n",
      "        [60, 51, 51, 50, 63,  1, 58, 50],\n",
      "        [46, 59, 49,  1, 46,  1, 53, 60],\n",
      "        [ 1, 58, 70,  0,  1, 61, 63, 50],\n",
      "        [46, 63, 49,  1, 60, 51,  1, 53]], device='cuda:0'), tensor([ 1, 65, 59, 50, 63, 49,  0,  5, 50, 53, 50, 63,  1, 18,  1, 64, 65,  1,\n",
      "        50, 53,  1, 65, 49,  1, 10, 50, 54, 65,  1,  8, 64, 50],\n",
      "       device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(train_dataloader):\n",
    "  print(batch)\n",
    "  if i == 1:\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Configuration\n",
    "VOCAB_SIZE = 74 # Should be set according to the tokenizer\n",
    "EMBED_DIM = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleGPT(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    # (B, T)\n",
    "    self.embedding = nn.Embedding(VOCAB_SIZE, VOCAB_SIZE) #(B, T, T)\n",
    "    \n",
    "\n",
    "  def forward(self, x, targets = None):\n",
    "    \"\"\"\n",
    "    x should be in the form of (B, T)\n",
    "    \"\"\"\n",
    "    logits = self.embedding(x)\n",
    "    y = logits[:, -1,:]\n",
    "    y = F.softmax(y, 1)\n",
    "\n",
    "    if targets is None:\n",
    "      loss = None\n",
    "    else:\n",
    "      targets_one_hot = F.one_hot(targets, VOCAB_SIZE).float()\n",
    "      loss = F.cross_entropy(y, targets_one_hot)\n",
    "\n",
    "    return y, loss\n",
    "  \n",
    "  def generate(self, x, max_tokens = 30):\n",
    "    \"\"\"\n",
    "    x (B, T)\n",
    "\n",
    "    ### returns\n",
    "    y (B, T + max_tokens)\n",
    "\n",
    "    ### Warning\n",
    "    Because we do not have an EOF, so it will generate max_tokens actually\n",
    "    \"\"\"\n",
    "    for _ in range(max_tokens):\n",
    "      # x shape (B, T)\n",
    "      probs, loss = self.forward(x) # y shape (B, vocab_size)\n",
    "      y = torch.multinomial(probs, num_samples=1)\n",
    "      x = torch.cat([x, y], dim = 1)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = SimpleGPT().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f17cd739bd0>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nM\\nnWR!FYcaB}?x ?P!Mb cqIl,nbI},U\"QQb:x[RnKY}.bd]n,MgKy,t_Rr\"\\'U`|zRqZGxC\"GYwM\\'_HdUl[!d hEwVJ;i`VAF e;F;FTtVfgbI[n|}A?gaYnEvNm}msyku|r<Htc IyP,?(,HtVFP]vDx??\\n`pSto`MKZd>Hw>D!oPIK[iUfOmS&\"eTj`oYSQ.N<_RFR'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mubble(max_tokens = 200):\n",
    "  input = torch.zeros(1, 1, dtype=torch.long,device=device)\n",
    "  output = gpt.generate(input, max_tokens)\n",
    "  text = tokenizer.decode(output[0].tolist())\n",
    "  return text\n",
    "\n",
    "mubble()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(gpt.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch():\n",
    "  for i, batch in enumerate(train_dataloader):\n",
    "    X, targets = batch\n",
    "    logits, loss = gpt.forward(X, targets)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % 1000 == 0:\n",
    "      print(loss)\n",
    "    \n",
    "    if i == 10000:\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.3069, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(4.2934, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(4.2760, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(4.2405, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(4.2337, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(4.1470, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(4.1357, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(4.0566, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(4.1399, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(4.0509, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(4.0919, device='cuda:0', grad_fn=<DivBackward1>)\n"
     ]
    }
   ],
   "source": [
    "epoch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " the the the the the the the che the the the hepe the the the the he the thand the the the ous t thand the ou the the the the tor the the the the f t the than the thouris the s the the he the the the \n"
     ]
    }
   ],
   "source": [
    "print(mubble())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
